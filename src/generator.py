import pandas as pd
from jinja2 import Environment, FileSystemLoader
import os
import shutil
import datetime

# ===========================
# 1. é…ç½®ä¸ç¿»è¯‘å­—å…¸
# ===========================
TRANSLATIONS = {
    'en': {
        'folder': '',
        'title_suffix': 'The Honest Review',
        'verdict_title': 'The Verdict',
        'check_price': 'Check Pricing',
        'price_chart': 'Price Comparison',
        'pros_hero': 'Advantages',
        'pros_comp': 'Advantages',
        'rated': 'Rated',
        'footer_rights': 'All rights reserved.',
        'col_pros': 'Pros', 'col_cons': 'Cons', 'col_verdict': 'Verdict',
        'home_btn': 'Read Review', 'privacy': 'Privacy Policy', 'terms': 'Terms of Use'
    },
    'es': {
        'folder': 'es',
        'title_suffix': 'OpiniÃ³n Honesta',
        'verdict_title': 'El Veredicto',
        'check_price': 'Ver Precios',
        'price_chart': 'ComparaciÃ³n de Precios',
        'pros_hero': 'Ventajas',
        'pros_comp': 'Ventajas',
        'rated': 'Calificado',
        'footer_rights': 'Todos los derechos reservados.',
        'col_pros': 'Pros_ES', 'col_cons': 'Cons_ES', 'col_verdict': 'Verdict_ES',
        'home_btn': 'Leer OpiniÃ³n', 'privacy': 'PolÃ­tica de Privacidad', 'terms': 'TÃ©rminos de Uso'
    },
    'pt': {
        'folder': 'pt',
        'title_suffix': 'AnÃ¡lise Honesta',
        'verdict_title': 'O Veredito',
        'check_price': 'Ver PreÃ§os',
        'price_chart': 'ComparaÃ§Ã£o de PreÃ§os',
        'pros_hero': 'Vantagens',
        'pros_comp': 'Vantagens',
        'rated': 'Avaliado',
        'footer_rights': 'Todos os direitos reservados.',
        'col_pros': 'Pros_PT', 'col_cons': 'Cons_PT', 'col_verdict': 'Verdict_PT',
        'home_btn': 'Ler AnÃ¡lise', 'privacy': 'PolÃ­tica de Privacidade', 'terms': 'Termos de Uso'
    }
}

# ç”¨äºæ”¶é›†æ‰€æœ‰é“¾æ¥ç”Ÿæˆ Sitemap
ALL_URLS = []

def generate_pages(csv_file, config):
    print("ğŸ­ [Generator V9.6] Building Multi-language Site with Sitemap...")
    
    base_output_dir = 'public'
    if os.path.exists(base_output_dir):
        shutil.rmtree(base_output_dir)
    os.makedirs(base_output_dir)
    
    # --- èµ„æºå¤åˆ¶ ---
    os.makedirs(f"{base_output_dir}/images", exist_ok=True)
    os.makedirs(f"{base_output_dir}/static", exist_ok=True)
    
    if os.path.exists('static'):
        for item in os.listdir('static'):
            s = os.path.join('static', item)
            d = os.path.join(f"{base_output_dir}/static", item)
            if os.path.isfile(s): shutil.copy2(s, d)

    if os.path.exists('data/images'):
        for img in os.listdir('data/images'):
            shutil.copy(f"data/images/{img}", f"{base_output_dir}/images/{img}")

    if not os.path.exists(csv_file): 
        print("âŒ CSV Not Found!")
        return

    df = pd.read_csv(csv_file).fillna("")
    env = Environment(loader=FileSystemLoader('templates'))
    tpl_compare = env.get_template('comparison.html')
    
    # å¦‚æœæœ‰ index.html æ¨¡æ¿å°±ç”¨ï¼Œæ²¡æœ‰å°±å¿½ç•¥ï¼ˆè¿™é‡Œå‡è®¾ä½ æœ‰ï¼‰
    try:
        tpl_index = env.get_template('index.html')
    except:
        tpl_index = None

    hero = config['hero_product']
    try:
        hero_data = df[df['Tool_Name'] == hero].iloc[0]
    except:
        print("âŒ Hero product not found in CSV")
        return

    # --- æ ¸å¿ƒå¾ªç¯ï¼šéå†ä¸‰ç§è¯­è¨€ ---
    for lang, trans in TRANSLATIONS.items():
        print(f"   ğŸŒ Generating {lang.upper()} pages...")
        
        # ç¡®å®šè·¯å¾„
        if trans['folder']:
            current_output_dir = f"{base_output_dir}/{trans['folder']}"
            url_prefix = f"{config['domain']}/{trans['folder']}"
        else:
            current_output_dir = base_output_dir
            url_prefix = f"{config['domain']}"
            
        os.makedirs(current_output_dir, exist_ok=True)

        # æ”¶é›†å½“å‰è¯­è¨€çš„æ‰€æœ‰é¡µé¢ï¼Œç”¨äºç”Ÿæˆè¯¥è¯­è¨€çš„é¦–é¡µ
        lang_pages_list = []

        # 1. ç”Ÿæˆå¯¹æ¯”é¡µ
        for index, row in df.iterrows():
            comp = row['Tool_Name']
            if comp == hero: continue
            
            slug = f"{hero.lower()}-vs-{comp.lower().replace(' ', '-')}"
            filename = f"{slug}.html"
            
            # æ•°æ®é€»è¾‘
            hero_pros = str(hero_data.get(trans['col_pros'], hero_data['Pros']))
            comp_pros = str(row.get(trans['col_pros'], row['Pros']))
            verdict_text = str(row.get(trans['col_verdict'], row['Verdict']))
            price_diff = float(row['Price']) - float(hero_data['Price'])
            reason = verdict_text if verdict_text else (f"Save ${int(price_diff)}/mo" if price_diff > 0 else "Great alternative")

            html = tpl_compare.render(
                config=config,
                hero=hero_data,
                comp=row,
                slug=slug,
                reason=reason,
                hero_pros=hero_pros,
                comp_pros=comp_pros,
                trans=trans,
                lang_code=lang
            )
            
            with open(f"{current_output_dir}/{filename}", "w", encoding="utf-8") as f:
                f.write(html)
            
            # è®°å½• URL åˆ° Sitemap å’Œ é¦–é¡µåˆ—è¡¨
            full_url = f"{url_prefix}/{filename}"
            ALL_URLS.append(full_url)
            lang_pages_list.append({'title': f"{hero} vs {comp}", 'link': filename})

        # 2. ç”Ÿæˆå½“å‰è¯­è¨€çš„ Index é¦–é¡µ
        if tpl_index:
            index_html = tpl_index.render(config=config, pages=lang_pages_list, trans=trans, lang_code=lang)
            with open(f"{current_output_dir}/index.html", "w", encoding="utf-8") as f:
                f.write(index_html)
            ALL_URLS.append(f"{url_prefix}/") # è®°å½•é¦–é¡µ URL

        # 3. ç”Ÿæˆç®€å•çš„ Privacy å’Œ Terms (é˜²æ­¢æ­»é“¾)
        # è¿™é‡Œç›´æ¥ç”Ÿæˆç®€å•çš„é™æ€ HTMLï¼Œä¸éœ€è¦æ¨¡æ¿ï¼Œä¿è¯åŠŸèƒ½å¯ç”¨
        privacy_content = f"""<html><head><title>{trans['privacy']}</title></head><body style="padding:20px; font-family:sans-serif;"><h1>{trans['privacy']}</h1><p>We use cookies to improve experience.</p><p><a href="index.html">Back to Home</a></p></body></html>"""
        with open(f"{current_output_dir}/privacy.html", "w", encoding="utf-8") as f:
            f.write(privacy_content)
        ALL_URLS.append(f"{url_prefix}/privacy.html")

        terms_content = f"""<html><head><title>{trans['terms']}</title></head><body style="padding:20px; font-family:sans-serif;"><h1>{trans['terms']}</h1><p>Standard terms apply.</p><p><a href="index.html">Back to Home</a></p></body></html>"""
        with open(f"{current_output_dir}/terms.html", "w", encoding="utf-8") as f:
            f.write(terms_content)
        ALL_URLS.append(f"{url_prefix}/terms.html")

    # --- 4. ç”Ÿæˆ CNAME ---
    if os.path.exists("CNAME"): shutil.copy("CNAME", f"{base_output_dir}/CNAME")

    # --- 5. ç”Ÿæˆ Robots.txt ---
    robots_txt = f"""User-agent: *
Allow: /
Sitemap: {config['domain']}/sitemap.xml
"""
    with open(f"{base_output_dir}/robots.txt", "w", encoding="utf-8") as f:
        f.write(robots_txt)
    print("âœ… Robots.txt generated.")

    # --- 6. ç”Ÿæˆ Sitemap.xml (æ ¸å¿ƒ) ---
    print(f"ğŸ—ºï¸ Generating Sitemap with {len(ALL_URLS)} URLs...")
    sitemap_content = '<?xml version="1.0" encoding="UTF-8"?>\n'
    sitemap_content += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    
    for url in ALL_URLS:
        # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„åŒæ–œæ  (é™¤ https:// å¤–)
        clean_url = url.replace('//', '/').replace('https:/', 'https://')
        sitemap_content += '  <url>\n'
        sitemap_content += f'    <loc>{clean_url}</loc>\n'
        sitemap_content += f'    <lastmod>{datetime.datetime.now().strftime("%Y-%m-%d")}</lastmod>\n'
        sitemap_content += '  </url>\n'
    
    sitemap_content += '</urlset>'
    
    with open(f"{base_output_dir}/sitemap.xml", "w", encoding="utf-8") as f:
        f.write(sitemap_content)
    
    print("âœ… Sitemap.xml generated successfully.")
    print("âœ… Full Site Build Complete.")

if __name__ == "__main__":
    # æ¨¡æ‹Ÿ Config è¿è¡Œ (Cloudflare ä¼šè°ƒç”¨ generate_pages)
    import json
    if os.path.exists('config.json'):
        with open('config.json', 'r') as f:
            config = json.load(f)
            generate_pages(f"data/{config['data_file']}", config)
    else:
        # æœ¬åœ°æµ‹è¯• fallback
        print("âš ï¸ No config.json found, checking local mode...")
